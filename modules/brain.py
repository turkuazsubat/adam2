from llama_cpp import Llama
import json
import datetime

class Brain:
    def __init__(self, model_path, tools_schema):
        print("ğŸ§  ADAM Beyni YÃ¼kleniyor... (Qwen-2.5-3B)")
        
        # Model YÃ¼kleme (RAM Dostu Ayarlar)
        self.llm = Llama(
            model_path=model_path,
            n_ctx=512,           # Context'i iyice dÃ¼ÅŸÃ¼rdÃ¼k (RAM'i kurtarÄ±r)
            n_batch=128,         # Paket boyutunu kÃ¼Ã§Ã¼lttÃ¼k
            n_threads=4,         # Ä°ÅŸlemciyi yormasÄ±n
            n_gpu_layers=0,      # Her ÅŸeyi CPU'ya Ã§ek (En gÃ¼venlisi bu)
            verbose=False
        )
        
        self.tools_schema = tools_schema
        print("âœ… Beyin Aktif.")

    def _create_system_prompt(self, user_profile):
        """
        Modelin kiÅŸiliÄŸini ve yeteneklerini tanÄ±mlayan prompt.
        JSON formatÄ±nÄ± zorlamak iÃ§in kurallar sÄ±kÄ±laÅŸtÄ±rÄ±ldÄ±.
        """
        current_time = datetime.datetime.now().strftime("%d %B %Y, %H:%M")
        
        prompt = f"""<|im_start|>system
Sen ADAM (Adaptive Personal Core). Python tabanlÄ±, yerel Ã§alÄ±ÅŸan geliÅŸmiÅŸ bir yapay zeka asistanÄ±sÄ±n.
Tarih: {current_time}

KULLANICI BÄ°LGÄ°SÄ°:
Ä°sim: {user_profile.get('name', 'KullanÄ±cÄ±')}
Biyografi: {user_profile.get('bio', 'Bilinmiyor')}
Ä°lgi AlanlarÄ±: {user_profile.get('interests', [])}

GÃ–REVLERÄ°N:
1. KullanÄ±cÄ±yla samimi, zeki ve doÄŸrudan bir dille konuÅŸ. Asla Wikipedia gibi sÄ±kÄ±cÄ± olma.
2. KullanÄ±cÄ±nÄ±n isteÄŸi bir eylem gerektiriyorsa (not almak, arama yapmak vb.), aÅŸaÄŸÄ±daki ARAÃ‡LARI (TOOLS) kullan.

MEVCUT ARAÃ‡LAR (TOOLS):
{self.tools_schema}

KRÄ°TÄ°K KURALLAR (Tool KullanÄ±mÄ±):
- EÄŸer bir araÃ§ kullanman gerekiyorsa, cevabÄ±n SADECE ÅŸu formatta olmalÄ±:
  <TOOL_CALL>{{"name": "tool_adi", "args": "parametre"}}</TOOL_CALL>
  
- Tool Ã§aÄŸrÄ±sÄ±ndan sonra veya Ã¶nce ASLA ekstra aÃ§Ä±klama yazma. Sadece XML/JSON ver.
- Parametre yoksa "args": null yap.
- EÄŸer sadece sohbet ediyorsan <TOOL_CALL> kullanma, normal cevap ver.
<|im_end|>
"""
        return prompt

    def think(self, user_input, user_profile, history=[]):
        """
        HAFIZA SINIRLAMALI DÃœÅÃœNME (Ã‡Ã¶kmeyi Ã–nler)
        """
        system_prompt = self._create_system_prompt(user_profile)
        
        messages = [{"role": "system", "content": system_prompt}]
        
        # --- KRÄ°TÄ°K DEÄÄ°ÅÄ°KLÄ°K: SADECE SON 2 MESAJI AL ---
        # 5'ten 2'ye dÃ¼ÅŸÃ¼rdÃ¼k ki 512 sÄ±nÄ±rÄ±nÄ± geÃ§mesin
        for msg in history[-2:]: 
            role = "user" if msg['role'] == "user" else "assistant"
            # Mesaj Ã§ok uzunsa kÄ±rp (HafÄ±za gÃ¼venliÄŸi iÃ§in)
            content = msg['content'][:200] 
            messages.append({"role": role, "content": content})

        # Yeni mesaj (Bunu da gÃ¼venlik iÃ§in biraz kÄ±sÄ±tlayalÄ±m)
        messages.append({"role": "user", "content": user_input[:300]})

        # Cevap Ãœret
        try:
            response = self.llm.create_chat_completion(
                messages=messages,
                temperature=0.6,
                max_tokens=256, # CevabÄ± da kÄ±sa tut ki sÄ±nÄ±r aÅŸÄ±lmasÄ±n
                stop=["<|im_end|>", "User:", "Siz:"]
            )
            return response['choices'][0]['message']['content']
        except Exception as e:
            # EÄŸer yine sÄ±nÄ±r hatasÄ± verirse, geÃ§miÅŸi tamamen silip sadece soruyu sor
            print(f"HafÄ±za doldu, temizleniyor... Hata: {e}")
            response = self.llm.create_chat_completion(
                messages=[{"role": "system", "content": system_prompt},
                          {"role": "user", "content": user_input}],
                max_tokens=256
            )
            return response['choices'][0]['message']['content']
    def process_tool_result(self, tool_result, history):
        """
        Tool Ã§alÄ±ÅŸtÄ±ktan sonra gelen sonucu (Ã¶rn: "Not kaydedildi") alÄ±r
        ve kullanÄ±cÄ±ya son bir nazik cevap Ã¼retir.
        """
        prompt = f"""<|im_start|>system
Sen ADAM. Az Ã¶nce bir aracÄ± baÅŸarÄ±yla Ã§alÄ±ÅŸtÄ±rdÄ±n.
AracÄ±n Teknik Ã‡Ä±ktÄ±sÄ±: {tool_result}

GÃ–REV:
Bu teknik Ã§Ä±ktÄ±yÄ± kullanÄ±cÄ±ya doÄŸal bir dille bildir.
Ã–rn: "Not kaydedildi" -> "TamamdÄ±r, notunu aldÄ±m."
Ã–rn: "Wikipedia: Polonya..." -> "AraÅŸtÄ±rdÄ±m ve ÅŸunu buldum: Polonya..."

KÄ±sa ve net ol.
<|im_end|>
"""
        response = self.llm.create_chat_completion(
            messages=[{"role": "system", "content": prompt}],
            temperature=0.7,
            max_tokens=200
        )
        return response['choices'][0]['message']['content']