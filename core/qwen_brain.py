"""
ADAM Merkezi Beyin - GGUF Qwen2.5 Model
Yerel, hÄ±zlÄ± ve hafÄ±za dostu LLM motoru
"""
import json
import logging
from typing import Dict, List, Optional
from llama_cpp import Llama

logger = logging.getLogger(__name__)


class QwenBrain:
    """
    GGUF formatÄ±nda Qwen modelini kullanan yerel LLM.
    Function calling ve baÄŸlamsal karar verme yeteneÄŸi.
    """
    
    def __init__(
        self,
        model_path: str = "models/qwen_agent.gguf",
        n_ctx: int = 4096,
        n_threads: int = 4,
        n_gpu_layers: int = 0
    ):
        """
        Args:
            model_path: GGUF model dosyasÄ±nÄ±n yolu
            n_ctx: Context window boyutu (4096 = ~3000 kelime)
            n_threads: CPU thread sayÄ±sÄ± (4-8 optimal)
            n_gpu_layers: GPU'ya yÃ¼klenecek katman sayÄ±sÄ± (0 = sadece CPU)
        """
        self.model_path = model_path
        
        logger.info(f"ğŸ§  Qwen Brain baÅŸlatÄ±lÄ±yor: {model_path}")
        
        try:
            self.llm = Llama(
                model_path=model_path,
                n_ctx=n_ctx,
                n_threads=n_threads,
                n_gpu_layers=n_gpu_layers,
                verbose=False  # Gereksiz log'larÄ± kapat
            )
            logger.info("âœ… Qwen Brain hazÄ±r (GGUF modu)")
        
        except Exception as e:
            logger.critical(f"Model yÃ¼kleme hatasÄ±: {e}")
            raise
    
    def generate_with_context(
        self,
        user_input: str,
        context: Dict,
        available_tools: List[Dict],
        max_tokens: int = 512,
        temperature: float = 0.7
    ) -> Dict:
        """
        BaÄŸlamsal Ã¼retim yapar ve tool Ã§aÄŸÄ±rma kararÄ± verir.
        
        Returns:
            {
                "intent": "command" | "query" | "chat",
                "tool_call": {
                    "name": "take_note",
                    "arguments": {"text": "..."}
                } | None,
                "response": "KullanÄ±cÄ±ya cevap"
            }
        """
        
        # Sistem prompt'unu oluÅŸtur
        system_prompt = self._build_system_prompt(context, available_tools)
        
        # Tam prompt'u hazÄ±rla
        full_prompt = f"""{system_prompt}

KullanÄ±cÄ±: {user_input}

Asistan (JSON formatÄ±nda cevap ver):"""
        
        logger.info(f"ğŸ’­ DÃ¼ÅŸÃ¼nÃ¼yor... (max {max_tokens} token)")
        
        try:
            # Model Ã¼retimi
            output = self.llm(
                full_prompt,
                max_tokens=max_tokens,
                temperature=temperature,
                top_p=0.9,
                stop=["KullanÄ±cÄ±:", "\n\n\n"],  # Durma koÅŸullarÄ±
                echo=False
            )
            
            # Ã‡Ä±ktÄ±yÄ± al
            generated_text = output['choices'][0]['text'].strip()
            
            # JSON ayrÄ±ÅŸtÄ±r
            result = self._parse_response(generated_text)
            
            logger.info(f"âœ… Karar: {result.get('intent')}")
            return result
        
        except Exception as e:
            logger.error(f"Ãœretim hatasÄ±: {e}")
            return {
                "intent": "chat",
                "tool_call": None,
                "response": "ÃœzgÃ¼nÃ¼m, bir dÃ¼ÅŸÃ¼nce hatasÄ± yaÅŸadÄ±m. Tekrar sÃ¶yler misin?"
            }
    
    def _build_system_prompt(self, context: Dict, tools: List[Dict]) -> str:
        """Dinamik sistem prompt'u oluÅŸturur"""
        
        profile = context.get("profile", {})
        user_name = profile.get("user_name", "KullanÄ±cÄ±")
        tone = profile.get("tone", "dostane")
        
        # Zaman bilgisi
        temporal = context.get("temporal", {})
        time_str = f"{temporal.get('day_of_week', 'BugÃ¼n')} {temporal.get('current_time', '')}"
        
        # KonuÅŸma geÃ§miÅŸi
        conversation = context.get("conversation", [])
        history_str = ""
        if conversation:
            for msg in conversation[-3:]:  # Son 3 konuÅŸma
                role = "KullanÄ±cÄ±" if msg["role"] == "user" else "Asistan"
                history_str += f"{role}: {msg['content']}\n"
        
        # Tool listesi
        tools_json = json.dumps(tools, indent=2, ensure_ascii=False)
        
        prompt = f"""Sen ADAM (Adaptive Personal Core), yerel Ã§alÄ±ÅŸan yapay zeka asistanÄ±sÄ±n.

KULLANICI BÄ°LGÄ°LERÄ°:
- Ä°sim: {user_name}
- Ãœslup Tercihi: {tone}
- Zaman: {time_str}

SON KONUÅMALAR:
{history_str if history_str else "Ä°lk etkileÅŸim"}

GÃ–REV:
KullanÄ±cÄ±nÄ±n isteÄŸini anla ve uygun aksiyonu belirle.

KULLANILABÄ°LÄ°R ARAÃ‡LAR:
{tools_json}

CEVAP FORMATI (SADECE JSON):
{{
  "intent": "command" veya "query" veya "chat",
  "tool_call": {{"name": "araÃ§_adÄ±", "arguments": {{"param": "deÄŸer"}}}} veya null,
  "response": "KullanÄ±cÄ±ya sÃ¶ylenecek kÄ±sa mesaj"
}}

KURALLAR:
1. AraÃ§ kullanÄ±lacaksa "tool_call" doldur, kullanÄ±lmayacaksa null yap
2. Her zaman geÃ§erli JSON formatÄ±nda cevap ver
3. Ãœslup '{tone}' olmalÄ±
4. KÄ±sa ve net cevaplar ver"""
        
        return prompt
    
    def _parse_response(self, raw_output: str) -> Dict:
        """Model Ã§Ä±ktÄ±sÄ±nÄ± JSON'a Ã§evirir"""
        
        try:
            # Markdown temizleme
            cleaned = raw_output.strip()
            if "```json" in cleaned:
                cleaned = cleaned.split("```json")[1].split("```")[0]
            elif "```" in cleaned:
                cleaned = cleaned.split("```")[1].split("```")[0]
            
            # JSON parse
            result = json.loads(cleaned.strip())
            
            # Validasyon
            if "intent" not in result:
                result["intent"] = "chat"
            if "response" not in result:
                result["response"] = cleaned[:200]
            
            return result
        
        except json.JSONDecodeError:
            logger.warning(f"JSON parse baÅŸarÄ±sÄ±z, ham Ã§Ä±ktÄ±: {raw_output[:100]}")
            
            # Fallback: Ham metni dÃ¶ndÃ¼r
            return {
                "intent": "chat",
                "tool_call": None,
                "response": raw_output[:500]
            }
    
    def simple_chat(self, message: str, max_tokens: int = 256) -> str:
        """
        Basit sohbet modu (tool kullanmadan).
        HÄ±zlÄ± cevaplar iÃ§in.
        """
        prompt = f"""Sen ADAM adlÄ± dostane bir asistansÄ±n.

KullanÄ±cÄ±: {message}
Asistan:"""
        
        try:
            output = self.llm(
                prompt,
                max_tokens=max_tokens,
                temperature=0.7,
                stop=["KullanÄ±cÄ±:"]
            )
            return output['choices'][0]['text'].strip()
        except Exception as e:
            logger.error(f"Basit sohbet hatasÄ±: {e}")
            return "ÃœzgÃ¼nÃ¼m, bir sorun oluÅŸtu."


# === TEST BLOÄU ===
if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    
    brain = QwenBrain()
    
    # Test 1: Basit sohbet
    print("=== TEST 1: Basit Sohbet ===")
    response = brain.simple_chat("Merhaba ADAM, nasÄ±lsÄ±n?")
    print(f"Cevap: {response}\n")
    
    # Test 2: Tool Ã§aÄŸÄ±rma
    print("=== TEST 2: Tool Ã‡aÄŸÄ±rma ===")
    context = {
        "profile": {"user_name": "Yavuz", "tone": "teknik"},
        "temporal": {"day_of_week": "Pazartesi", "current_time": "14:30"}
    }
    
    tools = [
        {
            "name": "take_note",
            "description": "Not alÄ±r",
            "parameters": {
                "type": "object",
                "properties": {
                    "text": {"type": "string"}
                }
            }
        }
    ]
    
    result = brain.generate_with_context(
        user_input="YarÄ±n doktora gideceÄŸimi not al",
        context=context,
        available_tools=tools
    )
    
    print(json.dumps(result, indent=2, ensure_ascii=False))